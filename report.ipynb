{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d74e1f-2157-4bc0-94e6-4f016717d82f",
   "metadata": {},
   "source": [
    "# <img src='./images/logo.svg' width=90 style=\"vertical-align:middle\" /> SHAREing: High-level performance assessment notebook\n",
    "\n",
    "This a template notebook for performing a high-level performance assessment, designed by the SHAREing consurtium. We see this notebook as a working document, where performance analysts can input measured data for a code, and use the markdown cells to make notes of their assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1475012e-0c83-41d4-bbe2-3474300df97f",
   "metadata": {},
   "source": [
    "## Software details (**in progress!**)\n",
    "\n",
    "We recommend the analyst provide key pieces of information (which currently contains dummy info):\n",
    "* Program name - **Tester**\n",
    "* Parallel model (e.g., OpenMP, MPI, etc.) - **OpenMP**\n",
    "* Compiler (including optimisation flags) - **gcc** with `-Ofast`\n",
    "* Libraries/dependencies - `hdf5`\n",
    "* Details on data input - using the `test_short.dat` configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f8fc10-7b6f-4d89-915d-81a9edbeb323",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b48f641-197b-4d79-9e79-b096e1afbd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from topics.core_table import core_perf\n",
    "from topics.intra_node import intra_node_perf\n",
    "from topics.inter_node import inter_node_perf\n",
    "from topics.gpu_table import gpu_perf\n",
    "from topics.io_table import io_perf\n",
    "from topics.summary_radar import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1755c-4bf1-4239-a2c0-77c2a4d3ee3f",
   "metadata": {},
   "source": [
    "### Core\n",
    "For a high-level core analysis we just want 2 measurements for a serial code:\n",
    "\n",
    "1. Maximum peak performance (Mflops/s)\n",
    "2. Measured average peak FLOPS (Mflops/s)\n",
    "\n",
    "**FLOPS** \\\n",
    "To measure these we use LIKWID, i.e., for FLOPS we use\n",
    "```bash\n",
    "likwid-bench -t peakflops -W S0:16kB:1\n",
    "likwid-perfctr -f -C 0 -g FLOPS_DP ./my_exe\n",
    "```\n",
    "in which the input data for the `peakflops` microbenchmark is half the L1 cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6a9f9-5046-45d1-b590-ea3abf613f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target peak per core (Mflops/s)\n",
    "maximum_performance = 7255.60\n",
    "# Measured average application peak (Mflops/s)\n",
    "measured_performance = 241.1913"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314f4ab-e26b-4b30-8c49-1776c5751fe7",
   "metadata": {},
   "source": [
    "We read these values into our `core_perf` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc6566-8363-478f-81f1-19a61f1971e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_performance_stats = core_perf(maximum_performance, measured_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a96f3-d718-4f45-9a50-8ee2b6104a98",
   "metadata": {},
   "source": [
    "Generate the **core performance** table below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e0ebb-d5e5-47b3-93ec-da137a42ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_performance_stats.core_perf_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68e7ed-7492-46fd-8725-03aca97ace8e",
   "metadata": {},
   "source": [
    "### Intra-node\n",
    "\n",
    "To quantify intra-node performance at a high level we reply simply on runtimes under a strong scaling analysis.\n",
    "\n",
    "**Serial run** \\\n",
    "If possible, we first measure the runtime of a serial application without any parallel libraries, e.g., compile without the `-fopenmp` flag. This can seem redundant but allows us to see the overhead of the parallel library when compared to a single-core run including the parallel library. \n",
    "\n",
    "If the parallel library cannot be switched off simply, then we suggest just setting the serial runtime equal to a single-core (with parallel library enabled) runtime.\n",
    "\n",
    "**Strong scaling** \\\n",
    "We now perform a strong scaling analysis by keeping our problem sized fixed but increasing the core count up to the maximum for your hardware. For an OpenMP code the thread number can be set simply with the `OMP_NUM_THREADS` environment variable, however, with this method thread affinity can be an issue. It can make performance variable relative to a thread pinned run.\n",
    "\n",
    "Thread pinning can be easily acheived by setting the `OMP_PROC_BIND` environment variable to `close`, however, we again make use of LIKWID\n",
    "```bash\n",
    "likwid-pin -c N:0-3 ./my_exe\n",
    "```\n",
    "This command can be nested into a for loop to increase the core count to efficiently perform a strong scaling analysis.\n",
    "\n",
    "**Input data** \\\n",
    "In the cell below we ask for the:\n",
    "1. Serial runtime\n",
    "2. A list of the core numbers used for the strong scaling\n",
    "3. A list of the relative runtime per number of cores\n",
    "\n",
    "The core count and runtimes are currently just setup with dummy data as lists. For significant strong scaling analyses, it can be quicker to save these data to a `*.csv` file and read these into lists rather than inputting values by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92695b5d-c3ec-48d1-934a-f3688244f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter serial performance time (s)\n",
    "serial_time = 162.22\n",
    "\n",
    "# enter number of cores in each trial\n",
    "number_of_cores = [1, 2, 3, 4, 6, 8, 12, 16]\n",
    "\n",
    "# Enter time for each number of cores (s)\n",
    "time = [162.22, # 1 core\n",
    "        55.61, # 2 core\n",
    "        42.85, # 3 core\n",
    "        35.35, # 4 core\n",
    "        30.82, # 6 core\n",
    "        24.45, # 8 core\n",
    "        21.93, # 12 core\n",
    "        19.72 # 16 core\n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a9342-26e8-4495-b721-0521d1e357ea",
   "metadata": {},
   "source": [
    "We read these values into our `intra_node_perf` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc628d6e-6e7d-448b-b181-8be9d822343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_node_stats= intra_node_perf(serial_time, number_of_cores, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78ce3c-8c53-4491-9047-276b6502c802",
   "metadata": {},
   "source": [
    "Generate the intra-node **parallel efficiency** plot below, including amber and red vertical lines which indicate the core counts below which the parallel efficiency drops to 80% and 60%, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b3327-6b42-4461-ac79-09fad77f29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_node_stats.parallel_efficiency_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de6e2af-3f02-49f9-b225-b953d39ae961",
   "metadata": {},
   "source": [
    "Generate the intra-node **runtimes** plot below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c8a9e-1682-456a-a76e-50070a733e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_node_stats.runtimes_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f90056e-4829-468c-afd2-cd2faa2ddfe0",
   "metadata": {},
   "source": [
    "Generate the **intra-node** performance table below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930fcd65-13b9-4f46-81c8-d63b65dc9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_node_stats.intra_node_perf_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92e6e06-f156-4086-b6c8-1fcc64c1a153",
   "metadata": {},
   "source": [
    "### Inter-node\n",
    "\n",
    "To quantify intra-node performance at a high level we reply simply on runtimes under a weak scaling analysis.\n",
    "\n",
    "**Weak scaling** \\\n",
    "To perform a weak scaling we run our executable across several number of nodes, e.g., 1, 2, 4, etc. For the problem size executed on 1 node we then need to scale up the problem for each run, i.e., double the problem size across for 2 nodes such that the problem size on each node remains approximately constant. \n",
    "\n",
    "**Input data** \\\n",
    "In the cell below we ask for the:\n",
    "1. A list of the node counts used for the weak scaling\n",
    "2. A list of the relative runtime per number of nodes\n",
    "\n",
    "The node count and runtimes are currently just setup with dummy data as lists. For significant weak scaling analyses, it can be quicker to save these data to a `*.csv` file and read these into lists rather than inputting values by hand.\n",
    "\n",
    "----------------\n",
    "#### TODO \n",
    "\n",
    "**Weak scaling (with LIKWID)** \\\n",
    "We simply increase the problem size with node count, i.e., double the nodes means double the problem size. Inter-node codes will depend on MPI as the communication library. In the intra-node case we had to be careful of *thread affinity* in the inter-node case we may suffer from low *rank affinity*. Once again LIKWID offers us tools to pin MPI ranks to physical nodes, and we recommend this workflow as performance results are more standardised and reliable.\n",
    "\n",
    "```bash\n",
    "likwid-mpirun -h\n",
    "```\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf8205-daba-431b-8665-6cb89a9acbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter serial performance time (s)\n",
    "inter_node_serial_time = 162.22\n",
    "\n",
    "# enter number of cores in each trial\n",
    "node_count = [1, 2, 4, 6, 8, 12, 16]\n",
    "\n",
    "# Enter time for each number of cores (s)\n",
    "inter_node_runtimes = [162.22, 160.0, 180.0, 200.0, 280.0, 220.0, 300.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f265d1f-987f-4a89-a2e2-ea62a4d644bb",
   "metadata": {},
   "source": [
    "We read these values into our `inter_node_perf` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15721d66-cc4b-4529-ae79-a6b15157dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_node_statistics = inter_node_perf(inter_node_serial_time, node_count, inter_node_runtimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299dc0ef-0d88-482b-b60b-1dfd1e322116",
   "metadata": {},
   "source": [
    "Generate the inter-node **parallel efficiency** plot below, including amber and red vertical lines which indicate the core counts below which the parallel efficiency drops to 80% and 60%, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda94741-813f-4a0e-8a9d-f5942454d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_node_statistics.parallel_efficiency_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e519b4-b51e-40fb-9c98-27bb821b1518",
   "metadata": {},
   "source": [
    "Generate the inter-node **runtimes** plot below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf122217-1365-4c66-bb16-bc90a4971693",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_node_statistics.runtimes_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8c0e3-f6a6-4e37-82e6-244ec74d95bc",
   "metadata": {},
   "source": [
    "Generate the **intra-node** performance table below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d8348-0903-44bb-940f-eb6bf5377f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_node_statistics.inter_node_perf_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a722d-e28d-4a54-b089-72182990cc64",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "To quantify GPU performance at a high level we use two metrics:\n",
    "1. GPU utilisation or occupancy\n",
    "2. GPU memory usage\n",
    "\n",
    "**GPU utilisation or occupancy** \\\n",
    "This metric is typically a measure of the proportion of computational resources in use by the code. In Nvidia language this will likely be the proportion of streaming multiprocessors in use. We simply read this metric into our `gpu_perf` class below, and most GPU vendor's tools will give this high-level metric for their hardware.\n",
    "\n",
    "**GPU memory usage** \\\n",
    "We compute the proportion of the maximum memory footprint used by the software by reading the theoretical `gpu_peak_memory` (typically from the vendor's data sheets) and measure the actual memory usage, `gpu_measured_memory`, which again is typically given from most vendor tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f2492-1fcd-43fa-ab51-8310aed9f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_utilisation = 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c80983-eab3-4928-8bba-26b721a18c83",
   "metadata": {},
   "source": [
    "We read these values into our `gpu_perf` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cd6d8-fc01-4ae4-9a2c-2fd693875fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_perf_statistics = gpu_perf(gpu_utilisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a39a4-79e9-4f30-be20-955a8f0b3530",
   "metadata": {},
   "source": [
    "Generate the **GPU performance** table below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a071f-2bbf-42f0-9822-2b35508ea19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_perf_statistics.gpu_perf_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ebdd8e-20f0-4275-8978-bccc7d24c5d5",
   "metadata": {},
   "source": [
    "### I/O\n",
    "\n",
    "To quantify I/O (Input/Output) performance at a high level we simply ask what proportion of an applications total runtime is spent in read and writes. Many performance tools can give these high-level metrics, e.g., Intel VTune and Linaro Performance Reports. Typically these measures are given for both reads and writes separately, so we here read these two proportions in independently, thus the performance is charactertised by $C_{\\mathrm{I/O}} = 1 - (C_{\\mathrm{read}} + C_{\\mathrm{write}})$. Therefore a low $C_{\\mathrm{I/O}}$ value implies that the application is spending significant time in reading and writing to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01705f97-2f12-4414-adf2-c37d79d56cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_proportion = 0.1\n",
    "write_proportion = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16649629-7832-419f-a418-00b2d1051481",
   "metadata": {},
   "source": [
    "We read these values into our `io_perf` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b2ea1c-df9c-4c79-8168-8ec4c31ed2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_statistics = io_perf(read_proportion, write_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f43e0b-c8ba-4ed1-9a35-7b6d30569b45",
   "metadata": {},
   "source": [
    "Generate the **I/O performance** table below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfdb431-ec7a-4771-8366-0d125b4cdd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "io_statistics.io_perf_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea1a1a-e293-43fe-abc4-ee5baca3fbb7",
   "metadata": {},
   "source": [
    "### Summary diagram (radar plot) - **In Progress!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb92503-d96b-40fd-8d3a-a1add72fb1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics = summary(core_performance_stats, intra_node_stats)\n",
    "summary_statistics.draw_radar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cdbe4f-4005-4cc8-9c7c-0c62c4c8c46e",
   "metadata": {},
   "source": [
    "This project has received funding through the UKRI Digital Research Infrastructure Programme under grant UKRI1801 (SHAREing)\n",
    "\n",
    "<img src='./images/ukri.png' width=200 style=\"vertical-align:middle\" /> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
